<div id="article_content" class="article_content clearfix">
        <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/ck_htmledit_views-1a85854398.css">
                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p>
<div class="toc">
 <h3><a name="t0"></a>文章目录</h3>
 <ul><li><a href="#_1" target="_self">一、机器学习任务攻略</a></li><li><ul><li><a href="#11_Framework_of_ML_2" target="_self">1.1 Framework of ML</a></li><li><a href="#12__9" target="_self">1.2 提高结果的准确性</a></li><li><ul><li><a href="#121_Loss_11" target="_self">1.2.1 Loss过大</a></li><li><a href="#122_Losstesting_dataLoss_25" target="_self">1.2.2 Loss足够小，但testing data的Loss过大</a></li></ul>
  </li></ul>
  </li><li><a href="#local_minimasaddle_point_69" target="_self">二、局部最小值（local minima）与鞍点（saddle point）</a></li><li><a href="#Batchmomentum_88" target="_self">三、Batch和momentum</a></li><li><ul><li><a href="#31_Batch_89" target="_self">3.1 Batch</a></li><li><a href="#32_Momentum_113" target="_self">3.2 Momentum</a></li></ul>
  </li><li><a href="#_123" target="_self">四、结语</a></li></ul>
</div>
<p></p> 
<h1><a name="t1"></a><a id="_1"></a>一、机器学习任务攻略</h1> 
<h2><a name="t2"></a><a id="11_Framework_of_ML_2"></a>1.1 Framework of ML</h2> 
<p>神经网络一共包含三个模块：训练模块、验证模块、预测模块。其中训练和验证模块共用数据是Training data，但要注意的是要把Training data分成训练和验证数据。<br> 训练步骤包括三个步骤：<br> 1、要先写出一个有未知参数的函数f（x），x为input，也叫做feature<br> 2、定义Loss函数，输入为一组参数，计算这组参数所造成的误差<br> 3、定义optimization，找到一组最为合适的参数θ*，使得Loss最小<br> 预测部分是用训练好的θ对Testing data进行预测结果。</p> 
<h2><a name="t3"></a><a id="12__9"></a>1.2 提高结果的准确性</h2> 
<p>检查training data的Loss</p> 
<h3><a name="t4"></a><a id="121_Loss_11"></a>1.2.1 Loss过大</h3> 
<p>显然在训练资料上效果并不是很好，这时要从两方面出发检查：<br> 1.model bias的原因：<br> model过于简单，导致在神经网络的函数集中没有可以让Loss变的足够小的函数，就像在海中捞针<br> 解决方法：重新设计model让其更加深，鲁棒性更强<br> 1.增加输入的feature，或者使用Deep Learning<br> <img src="https://img-blog.csdnimg.cn/fe38e307c8cc4583b1429e2ddb50f121.png" alt="在这里插入图片描述"><br> <img src="https://img-blog.csdnimg.cn/728166a8620e4c77871276b7949dcbff.png" alt="在这里插入图片描述"><br> 2.optimization做的不好<br> 在训练过程中可能卡到一个局部最优点（local minima）的地方，虽然网络中存在一个最好的函数，但是你无法找到让Loss最小的一组参数θ*<br> <img src="https://img-blog.csdnimg.cn/d9d0bbbebada476fb6ed815b5c7ffbfd.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 如何区分是model bias还是optimization的问题<br> 当一个神经网络的层数比另外一个神经网络要大，但Loss值却更大，则是optimization的问题，因为越深的网络的鲁棒性会更大；此外则是model bias的问题<br> <img src="https://img-blog.csdnimg.cn/091969533b60468e8be93969563b29a1.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p> 
<h3><a name="t5"></a><a id="122_Losstesting_dataLoss_25"></a>1.2.2 Loss足够小，但testing data的Loss过大</h3> 
<p>这里也要从两方面分析<br> 1.over fitting<br> 举一个极端的例子：<br> 我们的testing data为：<br> <img src="https://img-blog.csdnimg.cn/ad56387e9fb2422d9a9b7098bed4722d.png" alt="在这里插入图片描述"><br> 通过training找出这样一个函数：<br> <img src="https://img-blog.csdnimg.cn/d3bd1ecf77024e53b21edc839516b1b7.png" alt="在这里插入图片描述"><br> 当能够在训练资料中找到x的话就输出yi，否则就随机输出一个值。基于此，这个函数在training过程中的Loss为0，但是在test过程中就什么也没干，所以在testing过程中的Loss会很大，这就是over fitting。<br> 在真实的实验过程中，往往是因为model的鲁棒性太强，导致在training后得到的模型有很大的“自由区”而不能很好的拟合testing数据<br> <img src="https://img-blog.csdnimg.cn/98f7bbfa3dd943bcad15edc36787c35d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> <img src="https://img-blog.csdnimg.cn/e709d8efa02f4d139940caf0e27d6c28.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_12,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 导致Loss过大<br> 解决方法：<br> 1.增加数据量（Data augmentation）<br> <img src="https://img-blog.csdnimg.cn/bd247ec02e7642d2992d7cd19fe47027.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p> 
<p>2.限制model的鲁棒性<br> 根据training的数据设定函数，少一些参数<br> <img src="https://img-blog.csdnimg.cn/3b8b0f61c2ee40dfb5f1518cd5a8d6b6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 但要注意的是不能给model太多的限制，否则会导致model bias的问题<br> 综上，我们可以得出model的复杂程度和Loss的一个非线性关系<br> <img src="https://img-blog.csdnimg.cn/1564b47d4edb4778914f5d7175bd3d73.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 当model越来越复杂时，虽然Training loss越来越小，但是可能会出现over fitting的问题，导致预测结果不准<br> 我们在前面说过，我们有三大模块，训练、验证和预测，且将training data随机分成训练数据和验证数据，当我们在每一个epoch训练后得到的θ参数和loss，我们要将θ进行验证，如果验证组得到的loss更小，说明这组参数是较优的，将此loss替换训练得到的loss，最终找到一组最优解</p> 
<pre class="prettyprint"><code class="prism language-python has-numbering" onclick="mdcp.copyCode(event)" style="position: unset;">dev_mse <span class="token operator">=</span> dev<span class="token punctuation">(</span>model<span class="token punctuation">,</span> dev_data<span class="token punctuation">)</span>
<span class="token keyword">if</span> dev_mse <span class="token operator">&lt;</span> min_mse <span class="token punctuation">:</span>
   	min_mse <span class="token operator">=</span> dev_mse
<div class="hljs-button {2}" data-title="复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4259&quot;}"></div></code><ul class="pre-numbering" style=""><li style="color: rgb(153, 153, 153);">1</li><li style="color: rgb(153, 153, 153);">2</li><li style="color: rgb(153, 153, 153);">3</li></ul></pre> 
<pre class="prettyprint"><code class="prism language-python has-numbering" onclick="mdcp.copyCode(event)" style="position: unset;"><span class="token keyword">def</span> <span class="token function">dev</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> dev_data<span class="token punctuation">)</span> <span class="token punctuation">:</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    total_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> <span class="token builtin">input</span><span class="token punctuation">,</span> label <span class="token keyword">in</span> dev_data <span class="token punctuation">:</span>
        output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
        total_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>model<span class="token punctuation">.</span>cal_loss<span class="token punctuation">(</span>output<span class="token punctuation">,</span> label<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>total_loss<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>total_loss<span class="token punctuation">)</span>
<div class="hljs-button {2}" data-title="复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4259&quot;}"></div></code><ul class="pre-numbering" style=""><li style="color: rgb(153, 153, 153);">1</li><li style="color: rgb(153, 153, 153);">2</li><li style="color: rgb(153, 153, 153);">3</li><li style="color: rgb(153, 153, 153);">4</li><li style="color: rgb(153, 153, 153);">5</li><li style="color: rgb(153, 153, 153);">6</li><li style="color: rgb(153, 153, 153);">7</li></ul></pre> 
<p>此外我们还可以用N折交叉验证的方法进行分数据和找θ*<br> <img src="https://img-blog.csdnimg.cn/56cceab802cc4d93b098313d7c53a29d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 2.认为影响，这种问题不能怪model，机器是死的，这种问题一般来说叫做mismatch</p> 
<h1><a name="t6"></a><a id="local_minimasaddle_point_69"></a>二、局部最小值（local minima）与鞍点（saddle point）</h1> 
<p>当optimization失败时一般有两点原因：<br> 1.loss值无法再小，此时已经找到了一组最优解θ*<br> 2.gradient等于0<br> <img src="https://img-blog.csdnimg.cn/cf025c9accf44c99bfb743418a7b9b5d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述">这里我们对gradient==0的情况进行分析，当gradient等于0时，这个点有三种情况：<br> 处于local minima，此时loss处于局部最小，没办法走到其他地方<br> <img src="https://img-blog.csdnimg.cn/98b70267da1a402d9d966eb5d878d41f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_11,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 值得注意的是，local minima也有好坏之分当minima处于尖端的时候，testing loss是十分大的，而在平缓区时，testing loss就没那么大<br> <img src="https://img-blog.csdnimg.cn/f38512838eb842be82f213ad4557aabe.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p> 
<p>处于saddle point，此时loss并不是局部最小值，有别的地方可以走，这样就有机会到达全局最小点<br> <img src="https://img-blog.csdnimg.cn/056c72263b5a4d2bb8483b56a787b92a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_10,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> Saddle Point vs Local Minima<br> 他们两者之间谁更加常见呢？<br> 如果在从二维空间去看的话Saddle Point就可能被认为是Local Minima<br> <img src="https://img-blog.csdnimg.cn/dfdd51cc86e743838559786628dcbf56.png" alt="在这里插入图片描述"><br> 反过来假如我们从更高维度去看的话Local Minima却是Saddle Point<br> <img src="https://img-blog.csdnimg.cn/1c32aaf3bda34b058cd818e323cb639c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_13,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 也就是说假如我们在某个维度没路可走的时候，我们可以提高维度来使其变成Saddle Point从而有路可走。这就是现在为什么神经网络如此复杂、参数众多的重要原因</p> 
<h1><a name="t7"></a><a id="Batchmomentum_88"></a>三、Batch和momentum</h1> 
<h2><a name="t8"></a><a id="31_Batch_89"></a>3.1 Batch</h2> 
<p>现在假如说有一笔N资料，我们可以把N笔资料一次性全部跑完再计算loss和更新一次参数θ，但是的话我们也可以将N笔资料分成许多个batch资料，我们每跑完一个batch资料就计算更新一次参数<br> <img src="https://img-blog.csdnimg.cn/91949cf989944b53a2ce9d290e0a4d0c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 1 epoch等于把全部的batch都看一遍<br> 为什么要用batch？<br> 现在有20笔资料，我们分别看看用和不用batch的参数更新效果<br> <img src="https://img-blog.csdnimg.cn/2941be02cc9e4db5ae7b0e54028b59a6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 可以看出没有用batch的model的蓄力时间比较长，但每走一步都比较稳；用了batch的model蓄力时间短，但每次走的时候方是十分乱的。但是如果考虑gpu的平行预算，没有用batch耗费的时间不一定比用了batch所花时间长。在MNIST机器学习任务（一共有六万笔资料）中：<br> 跑每一个不同batch所用时间如下，（6000代表不用batch）<br> <img src="https://img-blog.csdnimg.cn/0d9369eca1ad471389b3fd91dc1d1430.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 我们再看跑完一个epoch和跑完一个batch所需时间对比：<br> <img src="https://img-blog.csdnimg.cn/028e4aa0dd214a7298b60a30dcb8d0ad.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 我们可以看出一个epoch大的batch花的时间反而是比较少的<br> 但是与我们直觉不同的是，分了batch的任务最终预测的正确率是比没分batch是要高的<br> <img src="https://img-blog.csdnimg.cn/3038616128e843e6bf6037f537bba320.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 为什么分batch会带来更好的结果？<br> 我们先来考虑Full batch的情况<br> 我们沿着Loss函数来更新参数，当陷入一个local minima之后就停止更新参数<br> <img src="https://img-blog.csdnimg.cn/575bfa93f08d475d8e346d92c3992239.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_12,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 再考虑small batch时，当我们用θ1参数组来算gradient的时候，他的loss函数是L1当遇到gradient等于0时就卡住了，但不会停止更新函数，可以用下一个batch来train优化model<br> <img src="https://img-blog.csdnimg.cn/924a99ef89964f72a4700479a13f88b4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> Small Batch VS Large Batch<br> <img src="https://img-blog.csdnimg.cn/7f28ab5d30a54fb5858ddbc5cf5cd6fa.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 正因他们有各自的优点，则Batch size变成一个hyper parameter（超参数）</p> 
<h2><a name="t9"></a><a id="32_Momentum_113"></a>3.2 Momentum</h2> 
<p>momentum是一门可能可以对抗local minima的技术。他的概念可以想象成物理世界中的惯性，我们可以想象，当一个小球沿着loss函数走，当走到local minima时因为他有惯性而不会在minima处停下<br> <img src="https://img-blog.csdnimg.cn/4ec31491daeb49c6bb65ae02b0b072fa.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 对于普通的gradient descent，假设开始在θ0的点，计算该点gradient为g(0)，沿着gradient的反方向移动到θ1 = θ0 - ng(0) 然后计算g(1)…<br> <img src="https://img-blog.csdnimg.cn/4a865146cedb4ad9a1df22327b4f4207.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 结合momentum的gradient descent<br> 我们在更新gradient时不单单沿着gradient的反方向，还要加上前一步(momentum)的方向进行更新，类似于物理中的力的合成，计算过程如下图：<br> <img src="https://img-blog.csdnimg.cn/2be951e89eed4036ac7c7ab29915a82f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 当我们走到local minima和和saddle point时虽然gradient==0，但是由于我们还有前一步的momentum所以还会继续走下去<br> <img src="https://img-blog.csdnimg.cn/9419e31cc1414edbb95afd1a247d221b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p> 
<h1><a name="t10"></a><a id="_123"></a>四、结语</h1> 
<p>以上是我机器学习学习笔记的第二篇，如有不对之处，还望指出，与君共勉。</p>
                </div><div data-report-view="{&quot;mod&quot;:&quot;1585297308_001&quot;,&quot;dest&quot;:&quot;https://blog.csdn.net/weixin_53598445/article/details/120177260&quot;,&quot;extend1&quot;:&quot;pc&quot;,&quot;ab&quot;:&quot;new&quot;}"><div></div></div>
                <link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-d7a94ec6ab.css" rel="stylesheet">
                <link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-49037e4d27.css" rel="stylesheet">
        </div>