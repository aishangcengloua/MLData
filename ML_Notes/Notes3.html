<div id="article_content" class="article_content clearfix">
        <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/ck_htmledit_views-1a85854398.css">
                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p>
<div class="toc">
 <h3><a name="t0"></a>文章目录</h3>
 <ul><li><a href="#Learning_rate_1" target="_self">一、Learning rate</a></li><li><ul><li><a href="#_2" target="_self">前言</a></li><li><a href="#11_learning_rate_9" target="_self">1.1 不同的参数需要不同的learning rate</a></li><li><ul><li><a href="#111_Root_Mean_Square_14" target="_self">1.1.1 Root Mean Square计算δ</a></li><li><a href="#112_RMSProp_20" target="_self">1.1.2 RMSProp计算δ</a></li></ul>
   </li><li><a href="#12__25" target="_self">1.2 检验结果</a></li></ul>
  </li><li><a href="#Classification_30" target="_self">二、Classification</a></li><li><a href="#Batch_Normalization_43" target="_self">Batch Normalization</a></li><li><a href="#_63" target="_self">三、结语</a></li></ul>
</div>
<p></p> 
<h1><a name="t1"></a><a id="Learning_rate_1"></a>一、Learning rate</h1> 
<h2><a name="t2"></a><a id="_2"></a>前言</h2> 
<p>顾名思义，就是要给每个参数不同的learning rate，上一篇笔记中，我们提到了在沿着Loss函数我们可能会陷入local minima等一些gradient为零从而导致参数无法更新，Loss也就不再下降。但事实是当Loss不再下降的时候，gradient不一定很小，如下图<img src="https://img-blog.csdnimg.cn/d01b9835e2264c7b9e81ed8e431e8824.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 当Loss很小时，gradient仍有在某时候是很大的，我们可以想象下面这样的情形导致的<img src="https://img-blog.csdnimg.cn/f5c537eea08244889a835027671b8d14.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_7,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 由图知道Loss不再下降并不是因为卡在local minima或者saddle point，而是因为此时的learning rate太大导致点更新的步伐过大使gradient在“山谷”两侧“反复横跳”导致Loss不再下降。再如下面这个例子：假设只有两个参数不断计算gradient来降低Loss<img src="https://img-blog.csdnimg.cn/9e68d8dd538a43ccb978343394cfb365.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_13,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 我们的目的就是能够使黑点到达黄色点的地方，此时Loss最小，设黄色点为山谷最低端，两旁是山壁，前文有说到当Loss不再下降时可能是因为“反复横跳”：<img src="https://img-blog.csdnimg.cn/3026ef9a40be4970a47e20184b7e2458.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 接下来可能就有一个疑问就是，为什么不把learning rate设低一点让黑点跨的步伐小一点进入中间地区呢？嗯，是个好疑问，我们把learning rate设为10e-7看看<img src="https://img-blog.csdnimg.cn/d30a77a1a71848ddb1e40d45d9149c34.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 从图可以看出，虽然黑点不再反复横跳，但他仍然不能到达黄点，这是因为此时的Loss函数已经十分平缓，而learning rate又太小而导致他不能继续再往前，所以我们需要更加特殊的gradient descent</p> 
<h2><a name="t3"></a><a id="11_learning_rate_9"></a>1.1 不同的参数需要不同的learning rate</h2> 
<p>当在某个地方的方向十分陡峭时我们就需要小的learning rate，反之需要大的learning rate<img src="https://img-blog.csdnimg.cn/ee5b846ca140495c89eb308077217486.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 按照普通的gradient descent，我们更新参数的方法是这样的<img src="https://img-blog.csdnimg.cn/74f768cc249f499aa97ef276c10e4ec4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_10,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 而现在我们因为需要同时更新learning rate，所以将learning rate除以依赖于某个对应参数的未知参数δ<img src="https://img-blog.csdnimg.cn/2950f70ec1ba48ed8fd06236df08722c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_10,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 下面来计算δ的值</p> 
<h3><a name="t4"></a><a id="111_Root_Mean_Square_14"></a>1.1.1 Root Mean Square计算δ</h3> 
<p><img src="https://img-blog.csdnimg.cn/e44708fa01b64c6eab2d79fdff90bf0d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 依照刚刚的δ的计算方法，当在Loss函数曲线较平缓的地方的gradient较小，因为δ跟gradient是呈正比关系，则算出来的δ也小，则learning rate较大，那便可以跨的步伐更大使Loss变化大一点<img src="https://img-blog.csdnimg.cn/d20cf402a93c48cfa6861b99059dd2e2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 在较陡的地方则相反<img src="https://img-blog.csdnimg.cn/036bf3a6f06b4ce7887944c753e1fe4a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_13,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 但是的话上面的方法还是不够好的，从上面的图中gradient的值都是差不多，都是朝着一个方向呈单调性变化的，但是在现实实验中我们可能会遇到的gradient在一个方向变化的程度可以是很大的<img src="https://img-blog.csdnimg.cn/b7a114266f664c81a52dfd3139dc1469.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 所以我们就需要能够动态改变learning rate的值</p> 
<h3><a name="t5"></a><a id="112_RMSProp_20"></a>1.1.2 RMSProp计算δ</h3> 
<p><img src="https://img-blog.csdnimg.cn/7f3d35962c424a00af9605f6b4f7b8c5.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> δ的值取决于gradient的大小，当gradient比较大的时候说明loss很陡峭需要刹车，所以δ的值也会变大，使得learning rate变小，达到一个刹车的目的，否则反之<br> <img src="https://img-blog.csdnimg.cn/df8ed34532034b039c27eda2e796004f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 我们结合RMSProp和Momentum就可以构成最常用的优化器：Adam</p> 
<h2><a name="t6"></a><a id="12__25"></a>1.2 检验结果</h2> 
<p>没有动态调整learning rate<br> <img src="https://img-blog.csdnimg.cn/158a6ed277ca46a68fb7946c78fe8180.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 之所以 出现红色圆圈里的现象是因为，随着gradient在y轴方向积累了很多小的值，使得δ在y轴方向很小导致learning rate太大出现井喷现象，但不会永远做简谐运动，因为随着learning rate还在不断更新黑点又会重新回到中间，我们有一种方法可以解决这个问题，就是Learning Rate Scheduling<img src="https://img-blog.csdnimg.cn/84200e0269764827aada97b055868bec.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 使随着时间的增加让learning rate变的小一些<img src="https://img-blog.csdnimg.cn/b3d8aef1a911433da070ddda757a4b78.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p> 
<h1><a name="t7"></a><a id="Classification_30"></a>二、Classification</h1> 
<p>在第一篇的笔记中有写过Regression的output是一个数值，Classification的output是一个类别，如果结合起来看，我们是否可以将Classification当作Regression去训练呢？<br> 这期间要求我们做得变化是将类别变成one-hot vector（独热向量）：<br> Class 1 = [1 0 0]T<br> Class 2 = [0 1 0]T<br> Class 3 = [0 0 1]T<br> 所以我们就能像做Regression一样得到三组数据<br> <img src="https://img-blog.csdnimg.cn/88ce9b37a3d74ef5a2d8e9a06e9398aa.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 最后将得到的y值经过softmax函数得到y’再计算与y^的距离<img src="https://img-blog.csdnimg.cn/843878427f184940a40d2e9337c2a743.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 因为经过神经网络我们得到的y值可能是任何值，但是我们的target只有0和1，所以需要做normalization（归一化）将y值限制到0-1之间，softmax的工作就是这个，具体过程如下：<br> <img src="https://img-blog.csdnimg.cn/20ffebbcfb72426590151b6691096fce.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 当我们在做两个类别分类时，可以直接使用sigmoid函数，因为sigmoid函数的取值范围是在0-1之间。<br> 对于Classification时我们常用的Loss函数是Cross-entropy，表达式为：Error = -∑y^i*lny’i，在pytorch中，Cross-entropy常常是与softmax结合在一起的。</p> 
<h1><a name="t8"></a><a id="Batch_Normalization_43"></a>Batch Normalization</h1> 
<p>首先我们先来看两个参数w1、w2对Loss的影响：<br> <img src="https://img-blog.csdnimg.cn/c0485ddfd05f430ebe212ec3ef195d32.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 且对应的神经网络是十分简单的：<br> <img src="https://img-blog.csdnimg.cn/920d7fb08e504d15a8ae92bbe04e7ad2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> x1改变一点后，最终L值会改变，但是由于x1的输入都很小时，其实对L的影响变不大。但是如果w2的输入很大，虽然w2只是改变一点点，最终也会对L造成很大影响<img src="https://img-blog.csdnimg.cn/e7d9ff9b971d477b8dfc48aa688d893f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 所以我们应该让不同维度的数值都有一个相同范围。<br> 假设R笔数据分布是这样的<br> <img src="https://img-blog.csdnimg.cn/428644aa65a24bf49e06a89e57264e2b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 我们先把同一维不同笔资料的mean计算出来，用那一维的数减去mean，再除以standard deviation（标准偏差）得到新的x’再放回原来位置，最终feature值都在0左右。考虑深度学习神经网络做normalization<br> <img src="https://img-blog.csdnimg.cn/79230d73a6854f4eab4d0c1c1710c1e0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 开始时我们对input vector做了normalization之后得到z1，z2，z3，而他们又是后一层网络的输入，但是我们得到z值的时候他们的分布并不是分布在0和1之间的，这样会导致第二层训练起来比较困难。所以我们要对z做normalization，一样的步骤，计算mean，std：<br> <img src="https://img-blog.csdnimg.cn/44a18541c94c4c13bc08443d391fa954.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> <img src="https://img-blog.csdnimg.cn/37d00de5708245989c58a6b02ede0642.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 上面这一大部分我们可以看成一层网络，这一层网络时是十分复杂的，因为里面有多个input和output，假如我们一次性把全部的资料读进去的话，计算量十分大，所以我们要考虑每次只对一部分的资料normalization，这就是Batch Normalization。值得注意的是Batch要足够多才行，只有这样才能算出u和δ的值<br> 上面都是训练过程，现在来看一下testing：<br> 上面我们知道u和 δ只能从比较大的Batch算出来，但是testing时，我们的数据量是不一定能够达到所需Batch值，<img src="https://img-blog.csdnimg.cn/d8c92380d5164808b2760b8e4f94a3c9.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5q275Zyo5rex5bqm5a2m5Lmg,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br> 这里有一个解决方法是，在训练时会把我们每次normalization得到的u1，u2，u3……拿出来计算得到平均值：<br> <img src="https://img-blog.csdnimg.cn/263bf80c401647f1becfa9084d788548.png" alt="在这里插入图片描述"><br> 所以我们在testing时就不用计算u和δ的值，直接拿平均值即可</p> 
<h1><a name="t9"></a><a id="_63"></a>三、结语</h1> 
<p>以上是我本人学习机器学习的学习笔记的第三篇，有错误的地方还望指出，共勉！</p>
                </div><div data-report-view="{&quot;mod&quot;:&quot;1585297308_001&quot;,&quot;dest&quot;:&quot;https://blog.csdn.net/weixin_53598445/article/details/120272373&quot;,&quot;extend1&quot;:&quot;pc&quot;,&quot;ab&quot;:&quot;new&quot;}"><div></div></div>
                <link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-d7a94ec6ab.css" rel="stylesheet">
                <link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-49037e4d27.css" rel="stylesheet">
        </div>